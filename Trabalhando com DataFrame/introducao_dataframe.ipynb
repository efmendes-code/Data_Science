{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6169853",
   "metadata": {},
   "source": [
    "# Introdução aos DataFrames – Python\n",
    "\n",
    "Este artigo demonstra várias APIs comuns do PySpark DataFrame usando Python.\n",
    "\n",
    "Um DataFrame é uma estrutura de dados rotulada bidimensional com colunas de tipos potencialmente diferentes. \n",
    "\n",
    "Você pode pensar em um DataFrame como uma planilha, uma SQL ou um dicionário de objetos de série. \n",
    "\n",
    "Para obter mais informações e exemplos, consulte o Início Rápido no site Apache Spark documentação.\n",
    "\n",
    "https://docs.microsoft.com/pt-br/azure/databricks/spark/latest/dataframes-datasets/introduction-to-dataframes-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16366bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
      "Collecting py4j==0.10.9.2\n",
      "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=298911f6c9fd6c9faefc7c23b81f317153086247e587c346e0704db2947381da\n",
      "  Stored in directory: c:\\users\\eduardo.mendes\\appdata\\local\\pip\\cache\\wheels\\2f\\f8\\95\\2ad14a4614b4a9f645ee928fbbd057b1b254c67adb494c9a58\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4f8c1",
   "metadata": {},
   "source": [
    "# Criar DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dafb8cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark class Row from module sql\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f0405d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Example Data - Departments and Employees\n",
    "\n",
    "# Create the Departments\n",
    "department1 = Row(id='123456', name='Computer Science')\n",
    "department2 = Row(id='789012', name='Mechanical Engineering')\n",
    "department3 = Row(id='345678', name='Theater and Drama')\n",
    "department4 = Row(id='901234', name='Indoor Recreation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "367bfdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Employees\n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\n",
    "employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
    "employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\n",
    "employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n",
    "employee5 = Employee('michael', 'jackson', 'no-reply@neverla.nd', 80000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f928d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\n",
    "departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
    "departmentWithEmployees3 = Row(department=department3, employees=[employee5, employee4])\n",
    "departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c20bcf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id='123456', name='Computer Science')\n"
     ]
    }
   ],
   "source": [
    "print(department1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77d9922e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000)\n"
     ]
    }
   ],
   "source": [
    "print(employee2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f64ffd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no-reply@berkeley.edu\n"
     ]
    }
   ],
   "source": [
    "print(departmentWithEmployees1.employees[0].email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d42270",
   "metadata": {},
   "source": [
    "# Criar DataFrames de uma lista de linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df642bba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\temp/ipykernel_5904/4140692742.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdepartmentsWithEmployeesSeq1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdepartmentWithEmployees1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepartmentWithEmployees2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdepartmentsWithEmployeesSeq1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\n",
    "df1 = spark.createDataFrame(departmentsWithEmployeesSeq1)\n",
    "\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e72fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
